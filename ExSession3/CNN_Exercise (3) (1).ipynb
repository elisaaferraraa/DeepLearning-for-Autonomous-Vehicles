{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb03R-oINjO0"
      },
      "source": [
        "## Convolutional Networks\n",
        "\n",
        "We'll check out how to build a **convolutional network** to classify CIFAR10 images. By using weight sharing - multiple units with the same weights - convolutional layers are able to learn repeated patterns in your data. For example, a unit could learn the pattern for an eye, or a face, or lower level features like edges.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8ZKW4STOlyI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "H0c3K5CGNjO6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as utils\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kpSL2MzAOWnk"
      },
      "outputs": [],
      "source": [
        "label_names = [\n",
        "    'airplane',\n",
        "    'automobile',\n",
        "    'bird',\n",
        "    'cat',\n",
        "    'deer',\n",
        "    'dog',\n",
        "    'frog',\n",
        "    'horse',\n",
        "    'ship',\n",
        "    'truck'\n",
        "]\n",
        "\n",
        "\n",
        "def plot_images(images, cls_true, cls_pred=None):\n",
        "    \"\"\"\n",
        "    Adapted from https://github.com/Hvass-Labs/TensorFlow-Tutorials/\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(3, 3)\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # plot img\n",
        "        ax.imshow(images[i, :, :, :], interpolation='spline16')\n",
        "\n",
        "        # show true & predicted classes\n",
        "        cls_true_name = label_names[cls_true[i]]\n",
        "        if cls_pred is None:\n",
        "            xlabel = \"{0} ({1})\".format(cls_true_name, cls_true[i])\n",
        "        else:\n",
        "            cls_pred_name = label_names[cls_pred[i]]\n",
        "            xlabel = \"True: {0}\\nPred: {1}\".format(\n",
        "                cls_true_name, cls_pred_name\n",
        "            )\n",
        "        ax.set_xlabel(xlabel)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYaAPNTtNjO8"
      },
      "outputs": [],
      "source": [
        "def get_train_valid_loader(data_dir='data',\n",
        "                           batch_size=64,\n",
        "                           augment=True,\n",
        "                           random_seed = 1,\n",
        "                           valid_size=0.02,\n",
        "                           shuffle=True,\n",
        "                           show_sample=False,\n",
        "                           num_workers=4,\n",
        "                           pin_memory=False):\n",
        "    \"\"\"\n",
        "    Utility function for loading and returning train and valid\n",
        "    multi-process iterators over the CIFAR-10 dataset. A sample\n",
        "    9x9 grid of the images can be optionally displayed.\n",
        "    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n",
        "    Params\n",
        "    ------\n",
        "    - data_dir: path directory to the dataset.\n",
        "    - batch_size: how many samples per batch to load.\n",
        "    - augment: whether to apply the data augmentation scheme\n",
        "      mentioned in the paper. Only applied on the train split.\n",
        "    - random_seed: fix seed for reproducibility.\n",
        "    - valid_size: percentage split of the training set used for\n",
        "      the validation set. Should be a float in the range [0, 1].\n",
        "    - shuffle: whether to shuffle the train/validation indices.\n",
        "    - show_sample: plot 9x9 sample grid of the dataset.\n",
        "    - num_workers: number of subprocesses to use when loading the dataset.\n",
        "    - pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n",
        "      True if using GPU.\n",
        "    Returns\n",
        "    -------\n",
        "    - train_loader: training set iterator.\n",
        "    - valid_loader: validation set iterator.\n",
        "    \"\"\"\n",
        "    error_msg = \"[!] valid_size should be in the range [0, 1].\"\n",
        "    assert ((valid_size >= 0) and (valid_size <= 1)), error_msg\n",
        "\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2023, 0.1994, 0.2010],\n",
        "    )\n",
        "\n",
        "    # define transforms\n",
        "    valid_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "    ])\n",
        "\n",
        "    #########################################################\n",
        "    #PART ADDED BY ME--------------------------------------------------------\n",
        "    if augment:\n",
        "      train_transform_original = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "      train_dataset_original = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=train_transform_original,\n",
        "      )\n",
        "\n",
        "      train_transform1 = transforms.Compose([\n",
        "            #transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "      train_dataset1 = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=train_transform1,\n",
        "       )\n",
        "\n",
        "      train_transform2 = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            #transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "      train_dataset2 = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=train_transform2,\n",
        "       )\n",
        "\n",
        "\n",
        "      valid_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=valid_transform,\n",
        "      )\n",
        "\n",
        "      train_dataset = torch.utils.data.ConcatDataset([train_dataset_original, train_dataset1,train_dataset2])\n",
        "      valid_dataset = torch.utils.data.ConcatDataset([valid_dataset,valid_dataset, valid_dataset])\n",
        "\n",
        "    else:\n",
        "      train_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "      # load the dataset\n",
        "      train_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=train_transform,\n",
        "      )\n",
        "\n",
        "      valid_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=valid_transform,\n",
        "      )\n",
        "\n",
        "\n",
        "    #END OF MY PART---------------------------------------------------------------\n",
        "\n",
        "    ################################################################################\n",
        "    \"\"\" #ORIGINAL CODE\n",
        "    if augment:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "    else:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "\n",
        "\n",
        "    # load the dataset\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=train_transform,\n",
        "    )\n",
        "\n",
        "    valid_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=valid_transform,\n",
        "    )\n",
        "    \"\"\"\n",
        "    ########################################################\n",
        "\n",
        "    num_train = len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=batch_size, sampler=valid_sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "\n",
        "    # visualize some images\n",
        "    if show_sample:\n",
        "        sample_loader = torch.utils.data.DataLoader(\n",
        "            train_dataset, batch_size=9, shuffle=shuffle,\n",
        "            num_workers=num_workers, pin_memory=pin_memory,\n",
        "        )\n",
        "        data_iter = iter(sample_loader)\n",
        "        images, labels = next(data_iter)\n",
        "        X = images.numpy().transpose([0, 2, 3, 1])\n",
        "        plot_images(X, labels)\n",
        "\n",
        "    return (train_loader, valid_loader)\n",
        "\n",
        "trainloader, valloader = get_train_valid_loader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1zuJ67-NjO_"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, n_input_channels=3, n_output=10):\n",
        "        super().__init__()\n",
        "        # Define convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        # Define batch normalization layers\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        #self.bn6 = nn.BatchNorm2d(256)\n",
        "        # Define max pooling layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(256 * 8 * 8, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        # Define dropout layer\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through convolutional layers\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu(self.bn5(self.conv6(x)))\n",
        "        # Flatten the output of the last convolutional layer\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "        # Forward pass through fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        logits = self.forward(x)\n",
        "        return F.softmax(logits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RZO3Ux0jz6r9"
      },
      "outputs": [],
      "source": [
        "# https://github.com/meng1994412/VGGNet_from_scratch/blob/master/pipeline/nn/conv/minivggnet.py\n",
        "# I took inspiration from the MiniVggNet implementation in the above link\n",
        "\n",
        "class MiniVGG(nn.Module):\n",
        "    def __init__(self, n_input_channels=3, n_output=10):\n",
        "        super(MiniVGG, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(n_input_channels, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn6 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(256, n_output)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool2(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = F.relu(self.bn5(self.fc1(x)))\n",
        "        x = self.dropout3(x)\n",
        "        x = F.relu(self.bn6(self.fc2(x)))\n",
        "        x = self.dropout3(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        logits = self.forward(x)\n",
        "        return F.softmax(logits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X95qKOZTPJ_A"
      },
      "outputs": [],
      "source": [
        "#Uncomment the net you want to use\n",
        "#net = ConvNet()\n",
        "net = MiniVGG()\n",
        "\n",
        "# use CUDA for training\n",
        "# Define the device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_workers = 1 if device.type == 'cuda' else 4\n",
        "pin_memory = True if device.type == 'cuda' else False\n",
        "# Load the data\n",
        "trainloader, valloader = get_train_valid_loader(num_workers=num_workers, pin_memory=pin_memory )\n",
        "\n",
        "# move all to device\n",
        "net.to(device)\n",
        "# Move trainloader and valloader to device\n",
        "trainloader = [(i. to(device), j. to(device)) for i, j in trainloader]\n",
        "valloader = [(i. to(device), j. to(device)) for i, j in valloader]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaPVL7OXp0gG"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Define the loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Specify the optimizer (SGD with learning rate of 0.01)\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "#optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=4, factor=0.1)\n",
        "\n",
        "# Initialize variables\n",
        "train_loss = 0.0\n",
        "valid_loss = 0.0\n",
        "epochs = 100\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "print_every = 80\n",
        "max_no_improve = 5  # Maximum number of epochs with no improvement\n",
        "no_improve_count = 0  # Counter for epochs with no improvement\n",
        "best_accuracy = 0.0  # Best validation accuracy\n",
        "\n",
        "# Initialize variables for storing accuracy values over epochs\n",
        "epoch_list = []\n",
        "accuracy_list = []\n",
        "lr_values = []\n",
        "\n",
        "for e in range(epochs):\n",
        "    start = time.time()\n",
        "    for images, labels in iter(trainloader):\n",
        "        steps += 1\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        output = net(images)\n",
        "        # Calculate the loss\n",
        "        loss = criterion(output, labels)\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Calculate validation accuracy at the end of each epoch\n",
        "    accuracy = 0\n",
        "    for images, labels in valloader:\n",
        "        predicted = net.predict(images).data\n",
        "        equality = (labels == predicted.max(1)[1])\n",
        "        accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
        "    accuracy /= len(valloader)\n",
        "\n",
        "    # Print accuracy and loss at the end of each epoch\n",
        "    print(\"Epoch: {}/{}..\".format(e + 1, epochs),\n",
        "          \"Loss: {:.4f}..\".format(running_loss / len(trainloader)),\n",
        "          \"Validation accuracy: {:.4f}\".format(accuracy))\n",
        "\n",
        "    # Append epoch number and accuracy to lists for plotting\n",
        "    epoch_list.append(e)\n",
        "    accuracy_list.append(accuracy)\n",
        "\n",
        "    # Check for early stopping based on validation accuracy\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        no_improve_count = 0\n",
        "    else:\n",
        "        no_improve_count += 1\n",
        "\n",
        "    # Check for early stopping\n",
        "    if no_improve_count >= max_no_improve:\n",
        "        print(\"Validation accuracy did not improve for {} epochs. Stopping training.\".format(max_no_improve))\n",
        "        break\n",
        "\n",
        "    running_loss = 0\n",
        "\n",
        "    # Update learning rate scheduler\n",
        "    scheduler.step(valid_loss)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    lr_values.append(current_lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUqGBbcpNjPB"
      },
      "source": [
        "Save best trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aruy-9K_S4Ay"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy over epochs\n",
        "plt.plot(epoch_list, accuracy_list, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Add grid\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot learning rate changes\n",
        "for epoch, lr in enumerate(lr_values):\n",
        "    if epoch > 0 and lr != lr_values[epoch - 1]:\n",
        "        plt.text(epoch, accuracy_list[epoch], f'LR: {lr:.5f}', fontsize=8, rotation=45)\n",
        "\n",
        "# Write last accuracy below the blue line\n",
        "last_accuracy = accuracy_list[-1]\n",
        "last_epoch = epoch_list[-1]\n",
        "plt.text(last_epoch, last_accuracy - 0.02, f'Last Accuracy: {last_accuracy:.4f}', fontsize=10, ha='right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TVt3x8SqAY4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5BKFS7ALNjPB"
      },
      "outputs": [],
      "source": [
        "#torch.save(net.state_dict(), 'drive/MyDrive/Colab Notebooks/CNN_ConvNet_final2.ckpt')\n",
        "torch.save(net.state_dict(), 'drive/MyDrive/Colab Notebooks/CNN_MiniVGG_deeperfully.ckpt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}