{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uGxnwhvlwMiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "864ba6d3-1b3b-458f-f2d4-ca2c3f144ada"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fPFXjAVFIKnh"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import platform\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vita-epfl/DLAV-2024.git"
      ],
      "metadata": {
        "id": "SwxcJW9wI9fp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef322f40-e8f4-4cba-b197-e2db1311f107"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DLAV-2024'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 58 (delta 10), reused 20 (delta 7), pack-reused 28\u001b[K\n",
            "Receiving objects: 100% (58/58), 27.96 MiB | 23.07 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the location of the saved weight relative to this notebook. Assume that they are in the same directory\n",
        "### Path to Model Weights\n",
        "path = './DLAV-2024/homeworks/hw2/test_batch'\n",
        "softmax_weights_path = '/content/drive/MyDrive/Colab Notebooks/softmax_weights_best.pkl'\n",
        "pytorch_weights_path = '/content/drive/MyDrive/Colab Notebooks/linearClassifier_pytorch_2ndVersDeeper_Dropout050_DataAugment5bis.ckpt'"
      ],
      "metadata": {
        "id": "pZXQTJIKJE_S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Copy your code from the Softmax Notebook to their corresponding function"
      ],
      "metadata": {
        "id": "mE6psT_aVPHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def softmax_loss_vectorized(W, X, y):\n",
        "    \"\"\"\n",
        "  Softmax loss function, vectorized version.\n",
        "  Inputs and outputs are the same as softmax_loss_naive.\n",
        "  \"\"\"\n",
        "    # Initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
        "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "    # regularization!                                                           #\n",
        "    #############################################################################\n",
        "    # Initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
        "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "    # here, it is easy to run into numeric instability.                         #\n",
        "    #############################################################################\n",
        "    n_samples = X.shape[0]\n",
        "\n",
        "    # Softmax\n",
        "    scores = X.dot(W)  # Rows\n",
        "    scores -= np.max(scores) # Numerical stability\n",
        "    exp_scores = np.exp(scores)\n",
        "    res = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    # Loss\n",
        "    L_i = -np.log(res[np.arange(n_samples), y])\n",
        "    loss = np.sum(L_i) / n_samples  # rows are the samples\n",
        "\n",
        "\n",
        "    # Gradient\n",
        "    probs = res.copy() # get the probability from softmax\n",
        "    # subract 1 to element corresponding to the correct class\n",
        "    probs[np.arange(n_samples), y] -= 1\n",
        "    probs /= n_samples\n",
        "    dW = X.T.dot(probs)\n",
        "    #############################################################################\n",
        "    #                          END OF YOUR CODE                                 #\n",
        "    #############################################################################\n",
        "\n",
        "    return loss, dW\n",
        "\n",
        "class LinearClassifier(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "\n",
        "    def train(self, X, y, learning_rate=1e-3, num_iters=30000,\n",
        "                batch_size=200, verbose=False):\n",
        "        \"\"\"\n",
        "        Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "          means that X[i] has label 0 <= c < C for C classes.\n",
        "        - learning_rate: (float) learning rate for optimization.\n",
        "        - num_iters: (integer) number of steps to take when optimizing\n",
        "        - batch_size: (integer) number of training examples to use at each step.\n",
        "        - verbose: (boolean) If true, print progress during optimization.\n",
        "\n",
        "        Outputs:\n",
        "        A list containing the value of the loss function at each training iteration.\n",
        "        \"\"\"\n",
        "\n",
        "        num_train, dim = X.shape\n",
        "        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
        "\n",
        "        if self.W is None:\n",
        "            # lazily initialize W\n",
        "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Sample batch_size elements from the training data and their           #\n",
        "            # corresponding labels to use in this round of gradient descent.        #\n",
        "            # Store the data in X_batch and their corresponding labels in           #\n",
        "            # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
        "            # and y_batch should have shape (batch_size,)                           #\n",
        "            #                                                                       #\n",
        "            # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
        "            # replacement is faster than sampling without replacement.              #\n",
        "            #########################################################################\n",
        "            num_train, dim = X.shape\n",
        "            num_classes = np.max(y) + 1\n",
        "            if self.W is None:\n",
        "                # lazily initialize W\n",
        "                self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "            # Run stochastic gradient descent to optimize W\n",
        "            loss_history = []\n",
        "            for it in range(num_iters):\n",
        "                #########################################################################\n",
        "                # TODO:                                                                 #\n",
        "                # Sample batch_size elements from the training data and their           #\n",
        "                # corresponding labels to use in this round of gradient descent.        #\n",
        "                # Store the data in X_batch and their corresponding labels in           #\n",
        "                # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
        "                # and y_batch should have shape (batch_size,)                           #\n",
        "                #                                                                       #\n",
        "                # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
        "                # replacement is faster than sampling without replacement.              #\n",
        "                #########################################################################\n",
        "\n",
        "                indexes = np.random.choice(num_train, batch_size, replace=True)\n",
        "                X_batch = X[indexes]\n",
        "                y_batch = y[indexes]\n",
        "            #########################################################################\n",
        "            #                       END OF YOUR CODE                                #\n",
        "            #########################################################################\n",
        "\n",
        "            # Evaluate loss and gradient\n",
        "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Update the weights using the gradient and the learning rate.          #\n",
        "            #########################################################################\n",
        "            self.W -= learning_rate * grad\n",
        "            #########################################################################\n",
        "            #                       END OF YOUR CODE                                #\n",
        "            #########################################################################\n",
        "\n",
        "            if verbose and it % 100 == 0:\n",
        "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
        "\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this linear classifier to predict labels for\n",
        "        data points.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "          array of length N, and each element is an integer giving the predicted\n",
        "          class.\n",
        "        \"\"\"\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO:                                                                   #\n",
        "        # Implement this method. Store the predicted labels in y_pred.            #\n",
        "        ###########################################################################\n",
        "         # for each sample, compute the loss for each class and the predicted class is the one with the lowest loss\n",
        "        scores = X.dot(self.W)\n",
        "        exp_scores = np.exp(scores)\n",
        "        res = exp_scores / np.sum(exp_scores, axis=1)[:, np.newaxis]\n",
        "        L_i = -np.log(res)\n",
        "        # for each row (sample), get the index of the class with the lowest loss\n",
        "        y_pred = np.argmin(L_i, axis=1)\n",
        "\n",
        "        ###########################################################################\n",
        "        #                           END OF YOUR CODE                              #\n",
        "        ###########################################################################\n",
        "        return y_pred\n",
        "\n",
        "    def loss(self, X_batch, y_batch):\n",
        "        \"\"\"\n",
        "        Compute the loss function and its derivative.\n",
        "        Subclasses will override this.\n",
        "\n",
        "        Inputs:\n",
        "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "          data points; each point has dimension D.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "\n",
        "\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to self.W; an array of the same shape as W\n",
        "\n",
        "         e = y_batch - np.dot(X_batch, self.W)\n",
        "\n",
        "        loss = np.dot(e.T, e)\n",
        "        grad = -np.dot(x_batch.T,e) / x_batch.shape[0]\n",
        "\n",
        "        return loss, grad\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "class Softmax(LinearClassifier):\n",
        "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
        "\n",
        "    def loss(self, X_batch, y_batch):\n",
        "        return softmax_loss_vectorized(self.W, X_batch, y_batch)"
      ],
      "metadata": {
        "id": "gHnLX6-oIkWm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Copy the model you created from the Pytorch Notebook"
      ],
      "metadata": {
        "id": "6chaH4G-Vfms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden1,n_hidden2,n_hidden3, n_output,dropout_prob=0.25):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Define 2 or more different layers of the neural network                      #\n",
        "        ################################################################################\n",
        "        self.hidden1 = torch.nn.Linear(n_feature, n_hidden1)\n",
        "        self.dropout1 = torch.nn.Dropout(dropout_prob)\n",
        "\n",
        "        self.hidden2 = torch.nn.Linear(n_hidden1, n_hidden2)\n",
        "        self.dropout2 = torch.nn.Dropout(dropout_prob)\n",
        "\n",
        "        self.hidden3 = torch.nn.Linear(n_hidden2, n_hidden3)\n",
        "        self.dropout3 = torch.nn.Dropout(dropout_prob)\n",
        "\n",
        "        self.predict = torch.nn.Linear(n_hidden3, n_output)\n",
        "        ################################################################################\n",
        "        #                              END OF YOUR CODE                                #\n",
        "        ################################################################################\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0),-1)\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Set up the forward pass that the input data will go through.                 #\n",
        "        # A good activation function betweent the layers is a ReLu function.           #\n",
        "        ################################################################################\n",
        "        x = self.hidden1(x)\n",
        "        #x = self.dropout1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.hidden2(x)\n",
        "        #x = self.dropout2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.hidden3(x)\n",
        "        #x = self.dropout3(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.predict(x)\n",
        "        ################################################################################\n",
        "        #                              END OF YOUR CODE                                #\n",
        "        ################################################################################\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "mSTfKTHEJBhy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: Follow the instructions in each of the following methods. **Note that these methods should return a 1-D array of size N where N is the number of data samples. The values should be the predicted classes [0,...,9].**\n",
        "\n"
      ],
      "metadata": {
        "id": "_UUbNTUAVsos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_usingPytorch(X):\n",
        "    #########################################################################\n",
        "    # TODO:                                                                 #\n",
        "    # - Create your model                                                   #\n",
        "    # - Load your saved model                                               #\n",
        "    # - Do the operation required to get the predictions                    #\n",
        "    # - Return predictions in a numpy array (hint: return \"argmax\")         #\n",
        "    #########################################################################\n",
        "    # create the net\n",
        "    net = Net(3072, 512, 256, 128, 10)\n",
        "    print(net)  # net architecture\n",
        "\n",
        "    # load the model\n",
        "    checkpoint = torch.load(pytorch_weights_path, map_location=torch.device('cpu'))\n",
        "    net.load_state_dict(checkpoint)\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():  # disable gradient calculation for efficiency during testing\n",
        "        scores = net(X)\n",
        "        y_pred = torch.argmax(scores, axis=1)\n",
        "\n",
        "\n",
        "    #########################################################################\n",
        "    #                       END OF YOUR CODE                                #\n",
        "    #########################################################################\n",
        "    return y_pred.numpy()\n",
        "\n",
        "def predict_usingSoftmax(X):\n",
        "    #########################################################################\n",
        "    # TODO:                                                                 #\n",
        "    # - Load your saved model into the weights of Softmax                   #\n",
        "    # - Do the operation required to get the predictions                    #\n",
        "    # - Return predictions in a numpy array                                 #\n",
        "    #########################################################################\n",
        "    # create and load the model\n",
        "    with open(softmax_weights_path, 'rb') as f:\n",
        "        W = pickle.load(f)\n",
        "    softmax = Softmax()\n",
        "    softmax.W = W.copy()\n",
        "\n",
        "    y_pred = softmax.predict(X)\n",
        "\n",
        "    #########################################################################\n",
        "    #                       END OF YOUR CODE                                #\n",
        "    #########################################################################\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "bEKafMuaI4By"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method loads the test dataset to evaluate the model."
      ],
      "metadata": {
        "id": "q8dM8fj39OBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Read DATA\n",
        "def load_pickle(f):\n",
        "    version = platform.python_version_tuple()\n",
        "    if version[0] == '2':\n",
        "        return  pickle.load(f)\n",
        "    elif version[0] == '3':\n",
        "        return  pickle.load(f, encoding='latin1')\n",
        "    raise ValueError(\"invalid python version: {}\".format(version))\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "  \"\"\" load single batch of cifar \"\"\"\n",
        "  with open(filename, 'rb') as f:\n",
        "    datadict = load_pickle(f)\n",
        "    X = datadict['data']\n",
        "    Y = datadict['labels']\n",
        "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "test_filename = path\n",
        "X,Y = load_CIFAR_batch(test_filename)"
      ],
      "metadata": {
        "id": "400u4eZNJAZq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet prepares the data for the different models. If you modify data manipulation in your notebooks, make sure to include them here."
      ],
      "metadata": {
        "id": "AJ3mBYnx9TIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Data Manipulation\n",
        "\n",
        "mean = np.array([0.4914, 0.4822, 0.4465])\n",
        "std = np.array([0.2023, 0.1994, 0.2010])\n",
        "X = np.divide(np.subtract( X/255 , mean[np.newaxis,np.newaxis,:]), std[np.newaxis,np.newaxis,:])\n",
        "\n",
        "X_pytorch = torch.Tensor(np.moveaxis(X,-1,1))\n",
        "X_softmax = np.reshape(X, (X.shape[0], -1))\n",
        "X_softmax = np.hstack([X_softmax, np.ones((X_softmax.shape[0], 1))])\n"
      ],
      "metadata": {
        "id": "IEmU5KnwJPBY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runs evaluation on the Pytorch and softmax model. **Be careful that *prediction_pytorch* and *prediction_softmax* are 1-D array of size N where N is the number of data samples. The values should be the predicted class [0,...,9]**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "O2nQbKPL9c3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Run Prediction\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "prediction_pytorch = predict_usingPytorch(X_pytorch)\n",
        "prediction_softmax = predict_usingSoftmax(X_softmax)\n",
        "\n",
        "## Run Evaluation\n",
        "acc_softmax = sum(prediction_softmax == Y)/len(X)\n",
        "acc_pytorch = sum(prediction_pytorch == Y)/len(X)\n",
        "print(\"Softmax= %f ... Pytorch= %f\"%(acc_softmax, acc_pytorch))\n"
      ],
      "metadata": {
        "id": "VKFPhm1wJjDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "667dacc0-c206-4729-b1b4-86b6092743ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (hidden1): Linear(in_features=3072, out_features=512, bias=True)\n",
            "  (dropout1): Dropout(p=0.25, inplace=False)\n",
            "  (hidden2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (dropout2): Dropout(p=0.25, inplace=False)\n",
            "  (hidden3): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (dropout3): Dropout(p=0.25, inplace=False)\n",
            "  (predict): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "Softmax= 0.402800 ... Pytorch= 0.560500\n"
          ]
        }
      ]
    }
  ]
}